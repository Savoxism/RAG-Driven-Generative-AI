{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12276.49s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==1.40.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.40.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.40.3) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.40.3) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.40.3) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.40.3) (0.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.40.3) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.40.3) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.40.3) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==1.40.3) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai==1.40.3) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai==1.40.3) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai==1.40.3) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.40.3) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai==1.40.3) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai==1.40.3) (2.18.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12282.62s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12288.65s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (0.13.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (75.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nguyenphuan/Library/Python/3.12/lib/python/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.2)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->spacy) (3.0.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/nguyenphuan/Library/Python/3.12/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==1.40.3\n",
    "!pip install python-dotenv\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12294.77s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# print(f\"Loaded API Key: {OPENAI_API_KEY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "client = OpenAI()\n",
    "gptmodel=\"gpt-4o\"\n",
    "start_time = time.time()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_with_full_text(itext):\n",
    "    text_input = '\\n'.join(itext)\n",
    "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=gptmodel,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
    "            )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def print_formatted_response(response):\n",
    "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
    "    wrapped_text = wrapper.fill(text=response)\n",
    "    print(\"Response:\")\n",
    "    print(\"---------------\")\n",
    "    print(wrapped_text)\n",
    "    print(\"---------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_records = [\n",
    "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
    "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
    "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
    "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
    "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
    "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
    "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
    "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
    "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
    "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
    "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
    "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
    "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
    "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
    "    \"The retrieved documents are then fed into the language model.\",\n",
    "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
    "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
    "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
    "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
    "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
    "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
    "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
    "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
    "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
    "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
    "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
    "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
    "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
      "in the field of artificial intelligence, particularly within the realm of\n",
      "natural language processing (NLP). It innovatively combines the capabilities of\n",
      "neural network-based language models with retrieval systems to enhance the\n",
      "generation of text, making it more accurate, informative, and contextually\n",
      "relevant. This methodology leverages the strengths of both generative and\n",
      "retrieval architectures to tackle complex tasks that require not only linguistic\n",
      "fluency but also factual correctness and depth of knowledge. At the core of\n",
      "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
      "transformer-based neural network, similar to those used in models like GPT\n",
      "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
      "Representations from Transformers). This component is responsible for producing\n",
      "coherent and contextually appropriate language outputs based on a mixture of\n",
      "input prompts and additional information fetched by the retrieval component.\n",
      "Complementing the language model is the retrieval system, which is usually built\n",
      "on a database of documents or a corpus of texts. This system uses techniques\n",
      "from information retrieval to find and fetch documents that are relevant to the\n",
      "input query or prompt. The mechanism of relevance determination can range from\n",
      "simple keyword matching to more complex semantic search algorithms which\n",
      "interpret the meaning behind the query to find the best matches. This component\n",
      "merges the outputs from the language model and the retrieval system. It\n",
      "effectively synthesizes the raw data fetched by the retrieval system into the\n",
      "generative process of the language model. The integrator ensures that the\n",
      "information from the retrieval system is seamlessly incorporated into the final\n",
      "text output, enhancing the model's ability to generate responses that are not\n",
      "only fluent and grammatically correct but also rich in factual details and\n",
      "context-specific nuances. When a query or prompt is received, the system first\n",
      "processes it to understand the requirement or the context. Based on the\n",
      "processed query, the retrieval system searches through its database to find\n",
      "relevant documents or information snippets. This retrieval is guided by the\n",
      "similarity of content in the documents to the query, which can be determined\n",
      "through various techniques like vector embeddings or semantic similarity\n",
      "measures. The retrieved documents are then fed into the language model. In some\n",
      "implementations, this integration happens at the token level, where the model\n",
      "can access and incorporate specific pieces of information from the retrieved\n",
      "texts dynamically as it generates each part of the response. The language model,\n",
      "now augmented with direct access to retrieved information, generates a response.\n",
      "This response is not only influenced by the training of the model but also by\n",
      "the specific facts and details contained in the retrieved documents, making it\n",
      "more tailored and accurate. By directly incorporating information from external\n",
      "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
      "are more factual and relevant to the given query. This is particularly useful in\n",
      "domains like medical advice, technical support, and other areas where precision\n",
      "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
      "systems can dynamically adapt to new information since they retrieve data in\n",
      "real-time from their databases. This allows them to remain current with the\n",
      "latest knowledge and trends without needing frequent retraining. With access to\n",
      "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
      "provide detailed and nuanced answers that a standalone language model might not\n",
      "be capable of generating based solely on its pre-trained knowledge. While\n",
      "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
      "with its challenges. These include the complexity of integrating retrieval and\n",
      "generation systems, the computational overhead associated with real-time data\n",
      "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
      "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
      "of the retrieved information remains a significant challenge, as does managing\n",
      "the potential for introducing biases or errors from the external sources. In\n",
      "summary, Retrieval Augmented Generation represents a significant advancement in\n",
      "the field of artificial intelligence, merging the best of retrieval-based and\n",
      "generative technologies to create systems that not only understand and generate\n",
      "natural language but also deeply comprehend and utilize the vast amounts of\n",
      "information available in textual form. A RAG vector store is a database or\n",
      "dataset that contains vectorized data points.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "paragraph = ' '.join(db_records)\n",
    "wrapped_text = textwrap.fill(paragraph, width=80)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"define a rag store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "Certainly! The content you've provided seems to be a request to define the term\n",
      "\"arag store.\" However, it appears there might be a typo or misunderstanding in\n",
      "the term \"arag store,\" as it doesn't correspond to a widely recognized concept\n",
      "or term in English.   If you meant \"arag store,\" it could be a misspelling or a\n",
      "specific term used in a particular context or language. Here are a few\n",
      "possibilities:  1. **Arg Store**: If you meant \"arg store,\" it could refer to a\n",
      "store or repository for arguments, possibly in a programming or computational\n",
      "context. In programming, \"args\" often refers to arguments passed to a function\n",
      "or command line.  2. **Arab Store**: If you meant \"Arab store,\" it could refer\n",
      "to a store that specializes in selling products from Arab countries or caters to\n",
      "the Arab community, offering items such as Middle Eastern foods, spices,\n",
      "clothing, or cultural artifacts.  3. **A Specific Brand or Name**: \"Arag\" could\n",
      "be a brand name, a specific store name, or a term used in a particular niche or\n",
      "community. In this case, more context would be needed to provide an accurate\n",
      "definition.  If you have more context or if there is a specific area or industry\n",
      "you are referring to, please provide additional details so I can give a more\n",
      "precise explanation.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(query)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Techniques and evaluation\n",
    "\n",
    "## 1. Retrieval metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        use_idf=True,\n",
    "        norm='l2',\n",
    "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "        sublinear_tf=True,   # Apply sublinear TF scaling\n",
    "        analyzer='word'      # You could also experiment with 'char' or 'char_wb' for character-level features\n",
    "    )\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nguyenphuan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        lemmatized_words.append(token.lemma_)\n",
    "    return lemmatized_words\n",
    "\n",
    "def expand_with_synonyms(words):\n",
    "    expanded_words = words.copy()\n",
    "    for word in words:\n",
    "        expanded_words.extend(get_synonyms(word))\n",
    "    return expanded_words\n",
    "\n",
    "def calculate_enhanced_similarity(text1, text2):\n",
    "    # Preprocess and tokenize texts\n",
    "    words1 = preprocess_text(text1)\n",
    "    words2 = preprocess_text(text2)\n",
    "\n",
    "    # Expand with synonyms\n",
    "    words1_expanded = expand_with_synonyms(words1)\n",
    "    words2_expanded = expand_with_synonyms(words2)\n",
    "\n",
    "    # Count word frequencies\n",
    "    freq1 = Counter(words1_expanded)\n",
    "    freq2 = Counter(words2_expanded)\n",
    "\n",
    "    # Create a set of all unique words\n",
    "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
    "\n",
    "    # Create frequency vectors\n",
    "    vector1 = [freq1[word] for word in unique_words]\n",
    "    vector2 = [freq2[word] for word in unique_words]\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive RAG\n",
    "\n",
    "### Keyword Search and Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Keyword Score: 3\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_best_match_keyword_search(query, db_records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "\n",
    "    # Split the query into individual keywords\n",
    "    query_keywords = set(query.lower().split())\n",
    "\n",
    "    # Iterate through each record in db_records\n",
    "    for record in db_records:\n",
    "        # Split the record into keywords\n",
    "        record_keywords = set(record.lower().split())\n",
    "\n",
    "        # Calculate the number of common keywords\n",
    "        common_keywords = query_keywords.intersection(record_keywords)\n",
    "        current_score = len(common_keywords)\n",
    "\n",
    "        # Update the best score and record if the current score is higher\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "\n",
    "    return best_score, best_record\n",
    "\n",
    "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
    "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
    "\n",
    "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.126\n"
     ]
    }
   ],
   "source": [
    "score = calculate_cosine_similarity(query, best_matching_record)\n",
    "print(f\"Best Cosine Similarity Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmented Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_input=query+ \": \"+ best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "An \"ARAG vector store\" refers to a specialized type of database or dataset that\n",
      "is designed to store and manage vectorized data points. Let's break down this\n",
      "concept further:  1. **Vectorized Data Points**: In the context of data science\n",
      "and machine learning, vectorization is the process of converting data into a\n",
      "numerical format that can be easily processed by algorithms. Each data point is\n",
      "represented as a vector, which is essentially an array of numbers. These vectors\n",
      "can represent various types of data, such as text, images, or any other form of\n",
      "information that can be encoded numerically.  2. **Purpose of a Vector Store**:\n",
      "The primary purpose of a vector store is to efficiently store and retrieve these\n",
      "vectorized data points. This is particularly useful in applications that require\n",
      "fast similarity searches, such as recommendation systems, image recognition, and\n",
      "natural language processing tasks. By storing data in a vectorized form, it\n",
      "becomes easier to perform operations like nearest neighbor searches, clustering,\n",
      "and classification.  3. **ARAG Vector Store**: While the term \"ARAG\" is not\n",
      "widely recognized in the context of vector stores, it could be a specific\n",
      "implementation or a proprietary system developed by a particular organization or\n",
      "for a specific use case. The key aspect is that it functions as a vector store,\n",
      "meaning it is optimized for handling vectorized data.  4. **Applications**:\n",
      "Vector stores are commonly used in machine learning and AI applications where\n",
      "the ability to quickly compare and analyze large sets of data is crucial. For\n",
      "example, in a recommendation system, a vector store can help identify items that\n",
      "are similar to a user's preferences by comparing the vector representations of\n",
      "items.  In summary, an ARAG vector store is a system designed to manage and\n",
      "utilize vectorized data efficiently, enabling advanced data processing and\n",
      "analysis capabilities in various applications.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match(text_input, records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "    for record in records:\n",
    "        current_score = calculate_cosine_similarity(text_input, record)\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "    return best_score, best_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_similarity_score, best_matching_record = find_best_match(query, db_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.126\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, best_matching_record)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_input=query+\": \"+best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "An ARAG vector store, or more generally a vector store, is a specialized type of\n",
      "database or dataset designed to store and manage vectorized data points. These\n",
      "data points are typically represented as high-dimensional vectors, which are\n",
      "numerical arrays that capture the essential features or characteristics of the\n",
      "data in a form that can be easily processed by machine learning algorithms and\n",
      "other computational methods.  Here's a more detailed explanation of the key\n",
      "components and concepts involved in a vector store:  1. **Vectorization**: This\n",
      "is the process of converting data into a numerical format that can be\n",
      "represented as vectors. For example, in natural language processing, text data\n",
      "can be vectorized using techniques like word embeddings (e.g., Word2Vec, GloVe)\n",
      "or sentence embeddings (e.g., BERT, Sentence Transformers). Similarly, images\n",
      "can be vectorized using convolutional neural networks (CNNs) to extract feature\n",
      "vectors.  2. **High-Dimensional Space**: Vectors are often high-dimensional,\n",
      "meaning they have many components or features. Each dimension represents a\n",
      "specific attribute or characteristic of the data. For instance, a vector\n",
      "representing a word might have hundreds of dimensions, each capturing a\n",
      "different aspect of its meaning or context.  3. **Storage and Retrieval**: A\n",
      "vector store is optimized for storing these high-dimensional vectors efficiently\n",
      "and allows for fast retrieval. This is crucial for applications that require\n",
      "quick access to similar data points, such as recommendation systems, image\n",
      "retrieval, or semantic search.  4. **Similarity Search**: One of the primary\n",
      "uses of a vector store is to perform similarity searches. This involves finding\n",
      "vectors that are close to a given query vector in the high-dimensional space.\n",
      "Techniques like nearest neighbor search (e.g., k-nearest neighbors, approximate\n",
      "nearest neighbors) are commonly used to identify similar vectors based on\n",
      "distance metrics like cosine similarity or Euclidean distance.  5.\n",
      "**Applications**: Vector stores are widely used in various fields, including\n",
      "natural language processing, computer vision, and recommendation systems. They\n",
      "enable efficient handling of large-scale data and support tasks like clustering,\n",
      "classification, and anomaly detection.  In summary, an ARAG vector store is a\n",
      "powerful tool for managing and utilizing vectorized data, enabling advanced data\n",
      "analysis and machine learning applications by providing efficient storage,\n",
      "retrieval, and similarity search capabilities.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def setup_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(records)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def find_best_match(query, vectorizer, tfidf_matrix):\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
    "    best_score = similarities[0, best_index]\n",
    "    return best_score, best_index\n",
    "\n",
    "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
    "\n",
    "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
    "best_matching_record = db_records[best_index]\n",
    "\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.407\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ability    access  accuracy  accurate     adapt  additional  advancement  \\\n",
      "0   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "1   0.000000  0.000000  0.000000  0.216364  0.000000    0.000000     0.000000   \n",
      "2   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "3   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.000000    0.236479     0.000000   \n",
      "5   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "7   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "8   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "9   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "10  0.186734  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "11  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "12  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "13  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "14  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "15  0.000000  0.172624  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "16  0.000000  0.317970  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "17  0.000000  0.000000  0.000000  0.206861  0.000000    0.000000     0.000000   \n",
      "18  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "19  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "20  0.000000  0.000000  0.000000  0.000000  0.275802    0.000000     0.000000   \n",
      "21  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "22  0.000000  0.174772  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "23  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "24  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "25  0.000000  0.000000  0.228743  0.000000  0.000000    0.000000     0.000000   \n",
      "26  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.173327   \n",
      "27  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "\n",
      "      advice  algorithms    allows  ...    vector  vectorized      when  \\\n",
      "0   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "1   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "2   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "3   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "4   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "5   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "6   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "7   0.000000    0.220687  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "8   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "9   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "10  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "11  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.295573   \n",
      "12  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "13  0.000000    0.000000  0.000000  ...  0.200131     0.00000  0.000000   \n",
      "14  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "15  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "16  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "17  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "18  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "19  0.244401    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "20  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "21  0.000000    0.000000  0.291503  ...  0.000000     0.00000  0.000000   \n",
      "22  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "23  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "24  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "25  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "26  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "27  0.000000    0.000000  0.000000  ...  0.307719     0.34589  0.000000   \n",
      "\n",
      "       where     which    while     wide      with    within   without  \n",
      "0   0.000000  0.000000  0.00000  0.00000  0.000000  0.260582  0.000000  \n",
      "1   0.000000  0.000000  0.00000  0.00000  0.160278  0.000000  0.000000  \n",
      "2   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "3   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "4   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "5   0.000000  0.247710  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "6   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "7   0.000000  0.179053  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "8   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "9   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "10  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "11  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "12  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "13  0.000000  0.182517  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "14  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "15  0.189283  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "16  0.000000  0.000000  0.00000  0.00000  0.258278  0.000000  0.000000  \n",
      "17  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "18  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "19  0.217430  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "20  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "21  0.000000  0.000000  0.00000  0.00000  0.192110  0.000000  0.291503  \n",
      "22  0.000000  0.000000  0.00000  0.21541  0.141963  0.000000  0.000000  \n",
      "23  0.000000  0.000000  0.32932  0.00000  0.217033  0.000000  0.000000  \n",
      "24  0.000000  0.000000  0.00000  0.00000  0.134513  0.000000  0.000000  \n",
      "25  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "26  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "27  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[28 rows x 297 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def setup_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(records)\n",
    "\n",
    "    # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(tfidf_df)\n",
    "\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "vectorizer, tfidf_matrix = setup_vectorizer(db_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_input=query+\": \"+best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "An ARAG vector store, or more generally a vector store, is a specialized type of\n",
      "database or dataset designed to store and manage vectorized data points. Let's\n",
      "break down the concept further:  1. **Vectorized Data Points**: In the context\n",
      "of machine learning and data science, data is often represented in the form of\n",
      "vectors. A vector is essentially an array of numbers that can represent various\n",
      "types of data, such as text, images, or any other form of information that can\n",
      "be numerically encoded. For example, in natural language processing, words or\n",
      "sentences can be transformed into vectors using techniques like word embeddings\n",
      "(e.g., Word2Vec, GloVe) or sentence embeddings.  2. **Purpose of a Vector\n",
      "Store**: The primary purpose of a vector store is to efficiently store,\n",
      "retrieve, and manage these high-dimensional vectors. This is particularly useful\n",
      "in applications that require similarity search, clustering, or any form of\n",
      "analysis that involves comparing data points based on their vector\n",
      "representations.  3. **Applications**: Vector stores are commonly used in\n",
      "various applications, including:    - **Recommendation Systems**: By comparing\n",
      "user preferences or item features in vector form, systems can recommend similar\n",
      "items.    - **Image and Text Retrieval**: Vectors can represent images or text,\n",
      "allowing for efficient retrieval of similar items based on content.    -\n",
      "**Natural Language Processing**: In NLP, vector stores can be used to manage\n",
      "embeddings for words, sentences, or documents, facilitating tasks like semantic\n",
      "search or topic modeling.  4. **Implementation**: Vector stores can be\n",
      "implemented using specialized databases or libraries that are optimized for\n",
      "handling high-dimensional data. Examples include FAISS (Facebook AI Similarity\n",
      "Search), Annoy (Approximate Nearest Neighbors Oh Yeah), and Elasticsearch with\n",
      "vector capabilities.  5. **Challenges**: Managing vector data involves\n",
      "challenges such as handling high-dimensional spaces, ensuring efficient storage\n",
      "and retrieval, and maintaining scalability as the dataset grows.  In summary, an\n",
      "ARAG vector store is a crucial component in modern data-driven applications,\n",
      "enabling efficient handling and analysis of vectorized data across various\n",
      "domains.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class RetrievalComponent:\n",
    "    def __init__(self, method='vector'):\n",
    "        self.method = method\n",
    "        if self.method == 'vector' or self.method == 'indexed':\n",
    "            self.vectorizer = TfidfVectorizer()\n",
    "            self.tfidf_matrix = None\n",
    "\n",
    "    def fit(self, records):\n",
    "      self.documents = records  # Initialize self.documents here\n",
    "      if self.method == 'vector' or self.method == 'indexed':\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        if self.method == 'keyword':\n",
    "            return self.keyword_search(query)\n",
    "        elif self.method == 'vector':\n",
    "            return self.vector_search(query)\n",
    "        elif self.method == 'indexed':\n",
    "            return self.indexed_search(query)\n",
    "\n",
    "    def keyword_search(self, query):\n",
    "        best_score = 0\n",
    "        best_record = None\n",
    "        query_keywords = set(query.lower().split())\n",
    "        for index, doc in enumerate(self.documents):\n",
    "            doc_keywords = set(doc.lower().split())\n",
    "            common_keywords = query_keywords.intersection(doc_keywords)\n",
    "            score = len(common_keywords)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_record = self.documents[index]\n",
    "        return best_record\n",
    "\n",
    "    def vector_search(self, query):\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]\n",
    "\n",
    "    def indexed_search(self, query):\n",
    "        # Assuming the tfidf_matrix is precomputed and stored\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
    "retrieval.fit(db_records)\n",
    "best_matching_record = retrieval.retrieve(query)\n",
    "\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.407\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity: 0.641582812483307\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(\"Enhanced Similarity:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_input=query+ \" \"+ best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A vector store, often referred to as a vector database or vector dataset, is a\n",
      "specialized type of database designed to store and manage data in the form of\n",
      "vectors. Vectors are mathematical representations of data points, typically in\n",
      "the form of multi-dimensional arrays or lists of numbers. These vectors are used\n",
      "to capture the essential features or characteristics of the data, enabling\n",
      "efficient storage, retrieval, and analysis.  Here are some key aspects of a\n",
      "vector store:  1. **Data Representation**: In a vector store, each data point is\n",
      "represented as a vector. This is particularly useful for handling complex data\n",
      "types such as images, text, or audio, where each data point can be transformed\n",
      "into a numerical vector using techniques like embeddings or feature extraction.\n",
      "2. **Similarity Search**: One of the primary uses of a vector store is to\n",
      "perform similarity searches. By comparing vectors, the database can quickly\n",
      "identify data points that are similar to a given query vector. This is commonly\n",
      "used in applications like recommendation systems, image recognition, and natural\n",
      "language processing.  3. **Dimensionality**: Vectors can have varying dimensions\n",
      "depending on the complexity and nature of the data. For example, word embeddings\n",
      "in natural language processing might be represented as 300-dimensional vectors,\n",
      "while image features might be represented in even higher dimensions.  4.\n",
      "**Scalability**: Vector stores are designed to handle large volumes of data\n",
      "efficiently. They often incorporate indexing techniques like KD-trees, ball\n",
      "trees, or more advanced methods like approximate nearest neighbor (ANN)\n",
      "algorithms to facilitate fast search and retrieval operations.  5.\n",
      "**Applications**: Vector stores are widely used in machine learning and\n",
      "artificial intelligence applications. They enable tasks such as clustering,\n",
      "classification, and anomaly detection by leveraging the geometric properties of\n",
      "vectors.  6. **Integration with Machine Learning**: Vector stores are often\n",
      "integrated with machine learning pipelines. They can store pre-computed\n",
      "embeddings generated by models, allowing for efficient downstream processing and\n",
      "analysis.  Overall, a vector store is a powerful tool for managing and analyzing\n",
      "data in a way that leverages the mathematical properties of vectors, making it\n",
      "an essential component in many modern data-driven applications.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_llm_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
